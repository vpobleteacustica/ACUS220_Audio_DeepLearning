<p align="left">
  <img src="escudo/uach_logo.png" alt="Universidad Austral de Chile" width="120" style="vertical-align:middle; margin-right:12px;">
</p>

# ACUS220 - AcÃºstica Computacional con Python
## Instituto de AcÃºstica - Universidad Austral de Chile
### Clasificando muestras de audio con deep learning (demo mÃ­nima)

Bienvenido/a. Esta guÃ­a te lleva paso a paso desde clonar el repositorio hasta entrenar un modelo CNN simple para clasificar seÃ±ales acÃºsticas.

> **Requisitos previos**: Git, Python 3.10+, y una de estas opciones para entornos: **Conda (recomendado)** o **venv + pip**.

---

## 1) Clonar el repositorio
```bash
git clone https://github.com/vpobleteacustica/ACUS220_Audio_DeepLearning.git
cd ACUS220_Audio_DeepLearning
```

Estructura mÃ­nima esperada:

```
ACUS220_Audio_DeepLearning/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # audios de entrada (tiny_dataset)
â”‚   â””â”€â”€ processed/          # se genera automÃ¡ticamente
â”œâ”€â”€ notebooks/              # cuadernos Jupyter opcionales
â”œâ”€â”€ scripts/                # scripts ejecutables (chequeo, dataset, train, infer)
â”œâ”€â”€ src/                    # utilidades y funciones de audio/features
â”œâ”€â”€ environment.yml         # entorno Conda (recomendado)
â”œâ”€â”€ requirements.txt        # alternativa con pip
â””â”€â”€ README.md
```

---

## 2) Crear y activar entorno Conda
```bash
conda env create -f environment.yml
conda activate audio_deeplearning
```

## 3) Verificar el entorno
```bash
python scripts/100_check_env.py
```

> Si aparece algÃºn error de importaciÃ³n, instala manualmente:
> ```bash
> pip install librosa soundfile torch torchvision torchaudio matplotlib pandas scikit-learn
> ```

Debe imprimir las versiones de librerÃ­as y **Shapes -> STFT/Logâ€‘Mel/MFCC**. Si ves â€œ**Entorno OK.**â€, vamos bien.

---

## 4) PARTE A â€“ Pipeline rÃ¡pido (sin splits, demo de la clase)

Esta parte usa todo tiny_dataset junto (sin separar en train/val/test). Es ideal para explicar conceptos bÃ¡sicos de:
	â€¢	extracciÃ³n de features,
	â€¢	visualizaciÃ³n de espectrogramas,
	â€¢	entrenamiento,
	â€¢	seeds,
	â€¢	e inferencia.

### 4A) Construir el dataset de features (sin splits)

Genera archivos `.npz` + `metadata.csv` desde `data/raw/tiny_dataset/`:

```bash
python scripts/200_build_tiny_dataset.py
```

Esto crea:

```
data/processed/tiny_specs/
â”œâ”€â”€ <clase>/*.npz
â””â”€â”€ metadata.csv
```

## Nota: Sobre los features extraÃ­dos:

La siguiente nota es vÃ¡lida tanto para `200_build_tiny_dataset.py`, como tambiÃ©n para `210_split_tiny_dataset.py`.

El script: `200_build_tiny_dataset.py`
	â€¢	Toma audios desde data/raw/tiny_dataset/
	â€¢	Genera todas las features:
	â€¢	STFT dB
	â€¢	Log-Mel dB
	â€¢	MFCC
	â€¢	Gammatone (opcional)
	â€¢	Guarda todo en los .npz
	â€¢	Pero la CNN sÃ³lo usa mel_db, que corresponde al Log-Mel spectrogram.

El script: `210_split_tiny_dataset.py`
	â€¢	Lee los audios, divide en train/val/test
	â€¢	Genera exactamente los mismos features por cada WAV, incluyendo:
	â€¢	STFT dB
	â€¢	Log-Mel dB
	â€¢	MFCC
	â€¢	Gammatone (si existe)
	â€¢	Y nuevamente, la CNN TinyMelCNN usa solamente mel_db (Log-Mel) al entrenar.



Â¿Por quÃ© el modelo CNN que entrenaremos utiliza sÃ³lo el feature Log-Mel? La respuesta breve es que:
	â€¢	Es la representaciÃ³n mÃ¡s estable para redes convolucionales,
	â€¢	Mantiene buena resoluciÃ³n temporal y espectral,
	â€¢	Es estÃ¡ndar en deep learning aplicado a audio.

ConclusiÃ³n: Aunque podemos generar varias representaciones, la CNN TinyMelCNN entrena Ãºnicamente con el Log-Mel spectrogram. TambiÃ©n, se podrÃ­a hacer que:
	â€¢	El modelo entrene con `MFCC`,
	â€¢	o con `STFT`,
	â€¢	o con `Gammatone`,
	â€¢	o incluso concatenar features (`multi-branch CNN`).

DeberÃ­as ver un resumen de conteo por clase y el archivo:
```
data/processed/tiny_specs/metadata.csv
```

## 5A) Visualizar ejemplos de espectrogramas Log-Mel

Esta figura es muy valiosa de analizar. La figura presenta ejemplos de espectrogramas Log-Mel (mel_db) para cada una de las cinco especies del tiny_dataset. Cada bloque corresponde a una clase distinta, y cada columna es un ejemplo diferente dentro de esa clase.

Â¿Por quÃ© es tan importante esta visualizaciÃ³n? Porque aquÃ­ podemos ver lo mismo que verÃ¡ la CNN cuando entreneâ€¦
y tambiÃ©n lo que no ve.

1. Lo que la CNN sÃ­ ve: Son patrones visuales.

La CNN interpreta cada espectrograma como si fuera una imagen.

Detecta cosas como:
	â€¢	Formas
	â€¢	Bordes
	â€¢	Texturas
	â€¢	Regiones de energÃ­a concentrada
	â€¢	Cambios bruscos o transiciones suaves
	â€¢	Patrones repetitivos
	â€¢	Manchas, lÃ­neas, franjas, pulsos, parches energÃ©ticos.

Es decir, la CNN observa morfologÃ­a energÃ©tica, no â€œsonidoâ€.

En la figura puedes notar:
	â€¢	Batrachyla leptopus â†’ patrones difusos, distribuidos en banda media
	â€¢	Batrachyla taeniata â†’ franja mÃ¡s estable en banda media-baja
	â€¢	Calyptocephalella gayi â†’ energÃ­a mÃ¡s baja, difusa
	â€¢	Pleurodema thaul â†’ pulsos verticales muy marcados
	â€¢	Porzana spiloptera â†’ estructuras mÃ¡s ruidosas y amplias

Cada especie tiene una â€œhuella visualâ€ distinta.

2. Lo que la CNN no sabe: Aunque tÃº veas â€œfrecuencias en Hz o kHzâ€, la CNN no tiene idea de eso.

La red no conoce:
	â€¢	quÃ© es un Hertz (Hz)
	â€¢	quÃ© es un 1 kHz
	â€¢	quÃ© parte del espectrograma es â€œagudoâ€ o â€œgraveâ€
	â€¢	quÃ© significa â€œfrecuencia fundamentalâ€, â€œarmÃ³nicosâ€ o â€œformantesâ€
	â€¢	quÃ© especie produce el sonido
	â€¢	quÃ© objeto fÃ­siolÃ³gico (trayecto vocal de la especie) generÃ³ la onda.

Para la CNN, el eje vertical no son kHz: es simplemente la coordenada Y de una imagen.

3. ConclusiÃ³n:

â€¢ Un espectrograma Log-Mel transforma un sonido en una imagen de energÃ­a.
â€¢ La CNN aprende a reconocer patrones visuales en esa imagen, no conceptos acÃºsticos como frecuencia, kHz, resonancia o timbre.
â€¢ Lo que aprende es la morfologÃ­a del sonido, tal como un ojo entrenado reconoce formas.

Por eso esta figura es tan valiosa: muestra claramente las â€œformas acÃºsticasâ€ que cada especie deja en su espectrograma, y revela por quÃ© una CNN es capaz de clasificarlas aun sin saber nada de acÃºstica.

```bash
python scripts/250_preview_tiny.py \
    --metadata data/processed/tiny_specs/metadata.csv \
    --feature mel_db \
    --n-per-class 5 \
    --out figures/tiny_specs_preview.png
```

Salida esperada:
```
data/figures/tiny_specs_preview.png
```
Debieras ver una imagen de 5 filas = 5 especies, por 5 columnas = 5 ejemplos de cada especie.


## 6A) Pipeline simple en un solo paso (opcional)

Ejecuta:
```bash
./scripts/2500_run_tiny_pipeline.sh
```

Este bash hace, en orden:
	1.	ConstrucciÃ³n del dataset (200_build_tiny_dataset.py)
	2.	VisualizaciÃ³n de ejemplos (250_preview_tiny.py)
	3.	Entrenamiento simple con Log-Mel (300_train_tiny_cnn.py)

## 7A) Entrenamiento simple (CNN pequeÃ±a con Log-Mel)

Â¿QuÃ© significan epochs, batch-size y learning rate? 

â€¢ Ã‰pocas (epochs)

Una Ã©poca es una pasada completa por todo el dataset de entrenamiento.
	â€¢	--epochs 25 significa que el modelo verÃ¡ 25 veces todos los ejemplos.
	â€¢	MÃ¡s Ã©pocas â†’ mÃ¡s aprendizaje (pero tambiÃ©n mÃ¡s riesgo de sobreajuste).
	â€¢	Menos Ã©pocas â†’ entrenamiento mÃ¡s rÃ¡pido pero tal vez insuficiente.

Una metÃ¡fora: Si estudias tu cuaderno completo una vez = 1 Ã©poca.

â€¢ TamaÃ±o de batch (batch-size)

El batch-size significa cuÃ¡ntos ejemplos procesa la red al mismo tiempo antes de actualizar los pesos.
	â€¢	--batch-size 8 significa que la red mira 8 espectrogramas por vez, calcula el error, y luego ajusta los pesos.
	â€¢	Batch pequeÃ±o â†’ aprendizaje mÃ¡s ruidoso pero mÃ¡s estable.
	â€¢	Batch grande â†’ aprendizaje mÃ¡s suave pero requiere mÃ¡s memoria.

Una metÃ¡fora: Estudias de a 8 ejercicios antes de revisar cÃ³mo vas.

â€¢ El learning rate controla quÃ© tan grande es el paso de aprendizaje en cada actualizaciÃ³n.
	â€¢	--lr 1e-3 significa un paso pequeÃ±o pero seguro.
	â€¢	Learning rate muy grande â†’ aprendizaje inestable.
	â€¢	Learning rate muy pequeÃ±o â†’ la red aprende muy lento.

Una metÃ¡fora: Si caminas hacia una meta con pasos muy grandes, puedes pasarte. En cambio, es mÃ¡s seguro llegar si tus pasos son pequeÃ±os.

# Resumen breve

| ParÃ¡metro | QuÃ© controla | Ejemplo | ExplicaciÃ³n|
|---------|-------------|---------|-------------|
| **epochs** | cuÃ¡ntas veces se recicla el dataset | 25 | estudiar el cuaderno 25 veces |
| **batch-size** | cuÃ¡ntos ejemplos se ven por paso| 8 | estudiar de a 8 ejercicios antes de corregir|
| **lr** | tamaÃ±o del paso de aprendizaje | 1e-3| pasos pequeÃ±os, estables |


Ejecuta entrenamiento con una o varias semillas (recomendado para reproducibilidad):
```bash
python -m scripts.300_train_tiny_cnn --epochs 25 --batch-size 8 --lr 1e-3 --seeds 42,7,123
```
---

Por cada seed se crea:
```
data/processed/tiny_specs/cnn_run/seed_<SEED>/
â”œâ”€â”€ best_model.pt
â”œâ”€â”€ history.csv              # pÃ©rdida y accuracy por Ã©poca
â”œâ”€â”€ curvas_loss_acc.png      # grÃ¡fico loss/accuracy train-val
â”œâ”€â”€ confusion_matrix.png     # matriz de confusiÃ³n (demo simple)
â””â”€â”€ class_names.json
```

Resumen general: 
```
data/processed/tiny_specs/cnn_run/run_summary.csv
```

Contiene, por seed: mejor Ã©poca, mejor val_acc, etc.

---

# Â¿QuÃ© significa control de aleatoriedad y uso de *semillas* (seed) en Deep Learning?

En modelos de deep learning, incluso cuando usamos exactamente el mismo script, los resultados pueden variar entre ejecuciones porque existen mÃºltiples fuentes internas de aleatoriedad.

---

## 1. InicializaciÃ³n aleatoria de pesos
Cada capa de la red neuronal inicia sus pesos con valores aleatorios.  
Una seed fija ese punto de partida.

---

## 2. Barajar el dataset
El `DataLoader` mezcla aleatoriamente los ejemplos antes de cada Ã©poca.  
Cambiar la seed cambia el orden â†’ cambia el gradiente â†’ cambia el entrenamiento.

---

## 3. AugmentaciÃ³n aleatoria
Rotaciones, ruidos, shifts, etc., son aplicados con azar.  
Sin seed, dos entrenamientos nunca verÃ¡n las mismas imÃ¡genes.

---

# 4 Â¿QuÃ© es entonces una *semilla* (seed)?

Una *seed* es un nÃºmero entero que **fija** todos los generadores de nÃºmeros aleatorios:

```python
import torch, numpy as np, random

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
```

Si usamos la misma seed â†’ obtendremos exactamente el mismo resultado.

---

# 5. Â¿Por quÃ© entrenar con **mÃºltiples** seeds?

Porque un Ãºnico entrenamiento puede ser afortunadoâ€¦ o no?.

Entrenar con diferentes semillas permite:

- Evaluar la **estabilidad** del modelo  
- Identificar si un resultado fue **casual**  
- Obtener mÃ©tricas mÃ¡s **robustas** (media Â± desviaciÃ³n estÃ¡ndar)  
- Comparar arquitecturas de forma **justa**.  

Ejemplo:

```bash
python -m scripts.300_train_tiny_cnn --epochs 25 --batch-size 8 --lr 1e-3 --seeds 42,7,123
```


# 6. MetÃ¡fora didÃ¡ctica.

Entrenar una red neuronal es como plantar un Ã¡rbol: Si cambiamos la semilla biolÃ³gica, el Ã¡rbol serÃ¡ similarâ€¦ pero nunca idÃ©ntico.

Varias seeds = varios Ã¡rboles â†’ puedes comparar cuÃ¡l creciÃ³ mejor.

# Resumen general

| Concepto | ExplicaciÃ³n |
|---------|-------------|
| **Seed** | NÃºmero que controla la aleatoriedad |
| **Reproducibilidad** | Misma seed â†’ mismo resultado |
| **Variabilidad natural** | Distintas seeds â†’ distintas curvas de entrenamiento |
| **Buena prÃ¡ctica** | Reportar media Â± desviaciÃ³n estÃ¡ndar |

---

## 8A) Inferencia: clasificar un WAV

Usa el mejor modelo del resumen (por defecto se elige el de **mÃ¡xima accuracy**):
```bash
python -m scripts.350_infer_one --wav data/raw/tiny_dataset/Batrachyla_taeniata/ejemplo.wav
```
selecciona tÃº el ejemplo.wav que quieras desde tus carpetas de audio por especie.

Salida esperada:
```
WAV: ...
Modelo: data/processed/tiny_specs/cnn_run/seed_123/best_model.pt
PredicciÃ³n: Batrachyla_taeniata (p=0.563)
Vista: data/processed/tiny_specs/cnn_run/seed_123/infer_*.png
```

> **Tip**: Para inferir por semilla especÃ­fica: `--seed 42`


## 9A) Inferencia por lote (batch)

Podemos clasificar todos los WAV de una carpeta y ver la distribuciÃ³n de predicciones:

```bash
python scripts/360_infer_batch.py \
    --wav-dir data/raw/tiny_dataset/Batrachyla_leptopus
```

Esto produce, para cada clase / carpeta que elijas:

```
data/processed/tiny_specs/cnn_run/seed_<SEED>/
â”œâ”€â”€ inference_results_<NOMBRE_CLASE>.csv    # predicciones por archivo
â”œâ”€â”€ summary_<NOMBRE_CLASE>.png              # grÃ¡fico de barras (clases predichas)
â””â”€â”€ inference_previews_<NOMBRE_CLASE>/*.png # espectrogramas + etiqueta modelo
```

---

## 1B) PARTE B â€“ Pipeline con train / val / test (configuraciÃ³n â€œseriaâ€)

En esta parte separamos explÃ­citamente los datos para:
	â€¢	entrenar (train),
	â€¢	ajustar hiperparÃ¡metros (val),
	â€¢	medir desempeÃ±o final (test).

Todo se hace de forma reproducible usando seeds.

### 1. Â¿Por quÃ© separar en train, val y test?

En aprendizaje profundo, nunca se entrena y evalÃºa un modelo con los mismos datos. Por eso dividimos el dataset en tres partes con roles muy distintos:

1. Train (entrenamiento)

Es el conjunto que la red usa directamente para aprender. AquÃ­ el modelo ajusta sus parÃ¡metros internos viendo miles de ejemplos.

Una metÃ¡fora: Imagina que eres un deportista de alto rendimiento, **train** serÃ­a tu rutina de ejercicios que haces durante una prÃ¡ctica.

2. Validation (val)

Usamos este split para **medir el rendimiento** durante el entrenamiento, sin afectar al modelo. Nos sirve para:
	â€¢	elegir hiperparÃ¡metros (lr, batch-size, n_mels, etc.),
	â€¢	decidir cuÃ¡ntas Ã©pocas entrenar,
	â€¢	seleccionar el mejor modelo (early stopping).

Tu metÃ¡fora: es el ensayo general que harÃ­as antes de tu competencia; no cuenta para el resultado deportivo que se trate, pero te indica cÃ³mo vas.

ğŸ”¹ 3. Test (evaluaciÃ³n final)

Este conjunto se usa una sola vez, al final. Nos mide el desempeÃ±o real del modelo en datos nunca antes vistos.

Tu metÃ¡fora: es la prueba oficial en tu competencia. Entras a la cancha y ahÃ­ realmente sabes quÃ© tan estÃ¡s, o en tÃ©rminos de un modelo de aprendizaje, quÃ© tan bien **generaliza** el modelo.

# Resumen general

| Split | Â¿QuÃ© es? | Â¿Para quÃ© sirve?|
|---------|-------------|-------------|
| **train** | datos usados para aprender | ajustar pesos del modelo |
| **val**   | datos para validar durante el entrenamiento | ajustar hiperparÃ¡metros y elegir el mejor modelo |
| **test** | datos NO usados en el entrenamiento | medir el rendimiento real, final y honesto |


## 2B) Crear splits del tiny_dataset (train/val/test)

A partir de data/raw/tiny_dataset/<clase>/*.wav:

Ejecuta:
```bash
python scripts/210_split_tiny_dataset.py`
```

Esto crea:

```
data/raw/tiny_dataset_split/
â”œâ”€â”€ train/<clase>/*.wav
â”œâ”€â”€ val/<clase>/*.wav
â””â”€â”€ test/<clase>/*.wav

data/raw/tiny_dataset_split/split_metadata.csv
```

El script va a imprimir cuÃ¡ntos archivos quedan en cada split y clase.

## 3B) Construir features por split

Ahora generamos .npz separados para cada split:

```bash
python scripts/220_build_tiny_dataset_from_split.py`
```

Salida:
```
data/processed/tiny_specs_split/
â”œâ”€â”€ train/<clase>/*.npz
â”œâ”€â”€ val/<clase>/*.npz
â”œâ”€â”€ test/<clase>/*.npz
â””â”€â”€ metadata_split.csv   # columnas: wav, npz, label, split
```

## 4B) Entrenamiento con splits (TinyMelCNN)

Entrenamos con **train**, validamos con **val** y evaluamos con **test**:

```bash
python scripts/300_train_tiny_cnn_split.py \
    --epochs 20 \
    --batch-size 16 \
    --seeds 42,7,123
```

Salida por seed:

```
data/processed/tiny_specs_split/cnn_run_split/seed_<SEED>/
â”œâ”€â”€ best_model.pt
â”œâ”€â”€ history.csv
â”œâ”€â”€ curvas_loss_acc.png          # loss/accuracy train vs val
â”œâ”€â”€ confusion_matrix_counts.csv  # matriz de confusiÃ³n (conteos)
â”œâ”€â”€ confusion_matrix_norm.csv    # matriz de confusiÃ³n (proporciones por fila)
â”œâ”€â”€ confusion_matrix.png         # figura con valores normalizados (0â€“1)
â”œâ”€â”€ test_metrics.json            # mÃ©tricas globales de test
â””â”€â”€ class_names.json
```

Resumen global de todas las seeds:

```bash
data/processed/tiny_specs_split/cnn_run_split/run_summary_split.csv
```

con columnas como: seed, best_epoch, best_val_acc, test_loss, test_acc, best_model.

*Comentario didÃ¡ctico*:
La **matriz de confusiÃ³n normalizada** (confusion_matrix_norm.csv + confusion_matrix.png) muestra, por cada clase real/verdadera, la distribuciÃ³n de probabilidades de predicciÃ³n.
Cada fila suma 1.0 â†’ se interpreta como:

â€œDado que la clase real es X, Â¿con quÃ© probabilidad el modelo predice Y?â€

## C) PARTE C: PrÃ³ximos pasos futuros
- Mapas de activaciÃ³n
- Gammatone filters
- MÃ¡s datos
- Modelos preentrenados (VGGish, YAMNet)
